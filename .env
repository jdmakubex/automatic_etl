# ---------- Proyecto ----------
COMPOSE_PROJECT_NAME=etl_prod

# ---------- Kafka (KRaft) ----------
# Genera uno si no tienes: docker compose run --rm kafka-1 /usr/bin/kafka-storage random-uuid
KAFKA_CLUSTER_ID=MkU3OEVBNTcwNTJENDM2Qk0000000000


# ---------- Click House ----------
CLICKHOUSE_HTTP_HOST=clickhouse
CLICKHOUSE_HTTP_PORT=8123
CLICKHOUSE_DATABASE=fgeo_analytics

# Superset (lectura)
CLICKHOUSE_USER=superset
CLICKHOUSE_PASSWORD=Sup3rS3cret!

# Ingestor (escritura)
CH_HTTP_HOST=clickhouse
CH_HTTP_PORT=8123
CH_DB=fgeo_analytics
CH_USER=etl
CH_PASSWORD=Et1Ingest!




# ---------- Superset (admin inicial) ----------
SUPERSET_ADMIN=admin
SUPERSET_PASSWORD=Admin123!
# Cambia por una cadena aleatoria larga (>= 32 chars) si la usas en tu compose
SUPERSET_SECRET_KEY=oL9TTCU5lrfEvvwQnDvRi2pWRrfDEUY0LHprgfvfROh19n6EJ7bURHdSu4Nd-yod1uP5G4kOGSV9BfMZ_qe-PA
DB_CONNECTIONS=[{"name":"default","host":"172.21.61.53","port":3306,"user":"juan.marcos","pass":"123456","db":"archivos"}]

# ---------- Conexiones de origen (JSON) ----------
# ESTE es el insumo de "configurator" para generar:
#  - .generated/clickhouse_init.sql  (CREATE DB/GRANT por cada fuente)
#  - .generated/superset_create_dbs.sh (conn en Superset a cada DB de CH)
# Puedes poner tantas entradas como quieras (MySQL/Postgres/etc).


# ---------- Kafka Connect / Debezium (si los usas) ----------
DEBEZIUM_CONNECT_URL=http://connect:8083
DBZ_SNAPSHOT_MODE=initial
DBZ_DECIMAL_MODE=string
DBZ_BINARY_MODE=base64
DBZ_TIME_PRECISION=connect
DBZ_SERVER_NAME_PREFIX=dbserver
DBZ_HISTORY_TOPIC=schema-changes

