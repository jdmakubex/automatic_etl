#!/usr/bin/env python3
"""
master_orchestrator.py
ORQUESTADOR MAESTRO CENTRALIZADO - Coordina toda la infraestructura ETL
EjecuciÃ³n recomendada: Docker (servicio maestro).
Maneja fases secuenciales con comunicaciÃ³n entre contenedores y validaciones robustas.
"""

import os
import sys
import json
import time
import logging
import subprocess
import requests
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from enum import Enum

# Estados de coordianaciÃ³n
class PhaseStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"

@dataclass
class PhaseResult:
    name: str
    status: PhaseStatus
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    duration_seconds: float = 0.0
    output: str = ""
    error: str = ""
    validation_passed: bool = False
    retry_count: int = 0
    container: str = ""
    dependencies_met: bool = False

class MasterOrchestrator:
    """Orquestador maestro que coordina todas las fases del pipeline ETL"""
    
    def __init__(self):
        self.start_time = datetime.now()
        self.logger = self._setup_logging()
        self.state_file = "/app/logs/orchestrator_state.json"
        self.phase_results: Dict[str, PhaseResult] = {}
        
        # ConfiguraciÃ³n de fases con dependencias explÃ­citas
        self.phases_config = {
            # FASE 1: INFRAESTRUCTURA BASE
            "infrastructure_validation": {
                "name": "ğŸ¥ ValidaciÃ³n de Infraestructura",
                "container": "etl-orchestrator", 
                "script": "tools/health_validator.py",
                "description": "Verifica que todos los servicios estÃ©n operativos",
                "dependencies": [],
                "timeout": 180,
                "required": True,
                "max_retries": 3,
                "validation_script": None
            },
            
            # FASE 2: CONFIGURACIÃ“N DE USUARIOS Y PERMISOS
            "user_setup": {
                "name": "ğŸ‘¥ ConfiguraciÃ³n de Usuarios",
                "container": "etl-orchestrator",
                "script": "tools/setup_database_users.py", 
                "description": "Crea usuarios y permisos en MySQL/ClickHouse",
                "dependencies": ["infrastructure_validation"],
                "timeout": 120,
                "required": True,
                "max_retries": 2,
                "validation_script": "tools/verify_dependencies.py"
            },
            
            # FASE 3: DESCUBRIMIENTO Y CONFIGURACIÃ“N DE ESQUEMAS
            "schema_discovery": {
                "name": "ğŸ” Descubrimiento de Esquemas", 
                "container": "etl-tools",
                "script": "tools/discover_mysql_tables.py",
                "description": "Detecta tablas MySQL y genera configuraciones",
                "dependencies": ["user_setup"],
                "timeout": 90,
                "required": True,
                "max_retries": 2,
                "validation_script": None
            },
            
            "clickhouse_models": {
                "name": "ğŸ—ï¸ CreaciÃ³n de Modelos ClickHouse",
                "container": "etl-tools", 
                "script": "tools/create_clickhouse_models.py",
                "description": "Crea tablas y esquemas en ClickHouse",
                "dependencies": ["schema_discovery"],
                "timeout": 120,
                "required": True,
                "max_retries": 2,
                "validation_script": "tools/validate_clickhouse.py"
            },
            
            # FASE 4: CONFIGURACIÃ“N DE CONECTORES
            "connector_generation": {
                "name": "âš™ï¸ GeneraciÃ³n de Pipeline",
                "container": "etl-tools",
                "script": "tools/gen_pipeline.py", 
                "description": "Genera configuraciones de conectores Debezium",
                "dependencies": ["clickhouse_models"],
                "timeout": 90,
                "required": True,
                "max_retries": 2,
                "validation_script": None
            },
            
            "connector_deployment": {
                "name": "ğŸ”Œ Despliegue de Conectores",
                "container": "etl-tools",
                "script": "tools/apply_connectors_auto.py",
                "description": "Aplica conectores Debezium en Kafka Connect", 
                "dependencies": ["connector_generation"],
                "timeout": 180,
                "required": True,
                "max_retries": 3,
                "validation_script": None
            },
            
            # FASE 5: INGESTA Y VALIDACIÃ“N FINAL
            "data_ingestion": {
                "name": "ğŸ“Š Ingesta de Datos",
                "container": "etl-tools",
                "script": "tools/ingest_runner.py",
                "description": "Ejecuta flujo de ingesta completo", 
                "dependencies": ["connector_deployment"],
                "timeout": 300,
                "required": True,
                "max_retries": 2,
                "validation_script": "tools/pipeline_status.py"
            },
            
            # FASE 6: CONFIGURACIÃ“N DE SUPERSET (OPCIONAL)
            "superset_setup": {
                "name": "ğŸ“ˆ ConfiguraciÃ³n de Superset",
                "container": "superset",
                "script": "tools/superset_auto_configurator.py",
                "description": "Configura dashboards y datasets en Superset",
                "dependencies": ["data_ingestion"], 
                "timeout": 180,
                "required": False,  # Opcional
                "max_retries": 2,
                "validation_script": None
            }
        }
        
        # Inicializar resultados de fases
        for phase_id, config in self.phases_config.items():
            self.phase_results[phase_id] = PhaseResult(
                name=config["name"],
                status=PhaseStatus.PENDING,
                container=config["container"]
            )
    
    def _setup_logging(self) -> logging.Logger:
        """Configurar logging maestro"""
        logger = logging.getLogger('MasterOrchestrator')
        logger.setLevel(logging.INFO)
        
        # Handler para archivo con rotaciÃ³n
        os.makedirs('/app/logs', exist_ok=True)
        file_handler = logging.FileHandler('/app/logs/master_orchestrator.log')
        file_handler.setLevel(logging.DEBUG)
        
        # Handler para consola  
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        
        # Formatters
        formatter = logging.Formatter(
            '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger
    
    def save_state(self):
        """Guardar estado actual del orquestador"""
        state_data = {
            "start_time": self.start_time.isoformat(),
            "current_time": datetime.now().isoformat(),
            "phase_results": {}
        }
        
        for phase_id, result in self.phase_results.items():
            state_data["phase_results"][phase_id] = {
                "name": result.name,
                "status": result.status.value,
                "start_time": result.start_time.isoformat() if result.start_time else None,
                "end_time": result.end_time.isoformat() if result.end_time else None,
                "duration_seconds": result.duration_seconds,
                "validation_passed": result.validation_passed,
                "retry_count": result.retry_count,
                "container": result.container,
                "dependencies_met": result.dependencies_met,
                "error": result.error[:500] if result.error else ""  # Truncar errores largos
            }
        
        try:
            with open(self.state_file, 'w') as f:
                json.dump(state_data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            self.logger.warning(f"No se pudo guardar estado: {e}")
    
    def check_dependencies(self, phase_id: str) -> bool:
        """Verificar que las dependencias de una fase estÃ©n completadas"""
        config = self.phases_config[phase_id]
        dependencies = config.get("dependencies", [])
        
        for dep_id in dependencies:
            if dep_id not in self.phase_results:
                self.logger.error(f"Dependencia '{dep_id}' no encontrada para fase '{phase_id}'")
                return False
            
            dep_result = self.phase_results[dep_id]
            if dep_result.status != PhaseStatus.COMPLETED:
                self.logger.warning(f"Dependencia '{dep_id}' no completada (estado: {dep_result.status.value})")
                return False
            
            if not dep_result.validation_passed and self.phases_config[dep_id].get("required", True):
                self.logger.error(f"Dependencia '{dep_id}' no pasÃ³ validaciÃ³n")
                return False
        
        return True
    
    def execute_script_in_container(self, container: str, script: str, timeout: int = 120) -> Tuple[bool, str, str]:
        """Ejecutar script en un contenedor especÃ­fico"""
        try:
            # Construir comando Docker
            cmd = [
                "docker", "compose", "exec", "-T", container,
                "python3", script
            ]
            self.logger.info(f"Ejecutando en '{container}': {' '.join(cmd[4:])}")
            # Usar wrapper para scripts principales si estÃ¡n disponibles
            script_basename = script.split('/')[-1]
            wrapper_scripts = [
                "health_validator.py", "setup_database_users.py", "discover_mysql_tables.py",
                "create_clickhouse_models.py", "gen_pipeline.py", "apply_connectors_auto.py",
                "ingest_runner.py", "superset_auto_configurator.py"
            ]
            if script_basename in wrapper_scripts:
                # Usar wrapper para coordinaciÃ³n automÃ¡tica
                cmd = [
                    "docker", "compose", "exec", "-T", container,
                    "python3", "tools/etl_script_wrapper.py", script_basename
                ]
                self.logger.info(f"Usando wrapper coordinado: {script_basename}")
            # Determinar cwd correcto
            # Si ejecutamos en un contenedor, el directorio de trabajo debe ser /app
            cwd = "/app"
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=timeout,
                cwd=cwd
            )
            success = result.returncode == 0
            return success, result.stdout, result.stderr
        except subprocess.TimeoutExpired:
            error_msg = f"Timeout ({timeout}s) ejecutando script en contenedor '{container}'"
            return False, "", error_msg
        except Exception as e:
            error_msg = f"Error ejecutando script: {str(e)}"
            return False, "", error_msg
    
    def validate_phase(self, phase_id: str) -> bool:
        """Ejecutar validaciÃ³n especÃ­fica de una fase"""
        config = self.phases_config[phase_id]
        validation_script = config.get("validation_script")
        
        if not validation_script:
            self.logger.debug(f"No hay script de validaciÃ³n para '{phase_id}'")
            return True  # Asumir Ã©xito si no hay validaciÃ³n especÃ­fica
        
        self.logger.info(f"ğŸ” Validando fase '{phase_id}' con '{validation_script}'")
        
        success, output, error = self.execute_script_in_container(
            container=config["container"],
            script=validation_script,
            timeout=60
        )
        
        if success:
            self.logger.info(f"âœ… ValidaciÃ³n de '{phase_id}' exitosa")
        else:
            self.logger.error(f"âŒ ValidaciÃ³n de '{phase_id}' fallÃ³: {error[:200]}")
        
        return success
    
    def comprehensive_phase_validation(self, phase_id: str) -> Tuple[bool, Dict[str, Any]]:
        """ValidaciÃ³n exhaustiva de una fase incluyendo estado, datos e integridad"""
        config = self.phases_config[phase_id]
        validation_results = {
            "phase": phase_id,
            "validations": {},
            "overall_success": False,
            "critical_issues": [],
            "warnings": []
        }
        
        self.logger.info(f"ğŸ” Iniciando validaciÃ³n exhaustiva para '{config['name']}'")
        
        try:
            # 1. ValidaciÃ³n bÃ¡sica con script especÃ­fico
            if config.get("validation_script"):
                basic_success = self.validate_phase(phase_id)
                validation_results["validations"]["basic_script"] = basic_success
                if not basic_success:
                    validation_results["critical_issues"].append("Script de validaciÃ³n bÃ¡sica fallÃ³")
            else:
                validation_results["validations"]["basic_script"] = True
            
            # 2. Validaciones especÃ­ficas por tipo de fase
            if phase_id == "infrastructure_validation":
                infra_success = self._validate_infrastructure_health()
                validation_results["validations"]["infrastructure_health"] = infra_success
                if not infra_success:
                    validation_results["critical_issues"].append("Servicios de infraestructura no saludables")
            
            elif phase_id == "user_setup":
                user_success = self._validate_database_connectivity()
                validation_results["validations"]["database_connectivity"] = user_success
                if not user_success:
                    validation_results["critical_issues"].append("Conectividad de base de datos fallÃ³")
            
            elif phase_id == "schema_discovery":
                schema_success = self._validate_schema_discovery()
                validation_results["validations"]["schema_discovery"] = schema_success
                if not schema_success:
                    validation_results["critical_issues"].append("Descubrimiento de esquemas fallÃ³")
            
            elif phase_id == "clickhouse_models":
                ch_success = self._validate_clickhouse_models()
                validation_results["validations"]["clickhouse_models"] = ch_success
                if not ch_success:
                    validation_results["critical_issues"].append("Modelos de ClickHouse no vÃ¡lidos")
            
            elif phase_id == "connector_deployment":
                connector_success = self._validate_connectors()
                validation_results["validations"]["connectors"] = connector_success
                if not connector_success:
                    validation_results["critical_issues"].append("Conectores Debezium no funcionando")
            
            elif phase_id == "data_ingestion":
                data_success = self._validate_data_flow()
                validation_results["validations"]["data_flow"] = data_success
                if not data_success:
                    validation_results["critical_issues"].append("Flujo de datos no operativo")
            
            # 3. Determinar Ã©xito general
            critical_failures = len(validation_results["critical_issues"])
            total_validations = len(validation_results["validations"])
            successful_validations = sum(validation_results["validations"].values())
            
            # Criterio: al menos 80% de validaciones exitosas y sin issues crÃ­ticos
            success_rate = (successful_validations / total_validations) if total_validations > 0 else 0
            validation_results["overall_success"] = success_rate >= 0.8 and critical_failures == 0
            
            if validation_results["overall_success"]:
                self.logger.info(f"âœ… ValidaciÃ³n exhaustiva exitosa para '{phase_id}' ({success_rate:.1%})")
            else:
                self.logger.error(f"âŒ ValidaciÃ³n exhaustiva fallÃ³ para '{phase_id}': {critical_failures} issues crÃ­ticos")
            
            return validation_results["overall_success"], validation_results
            
        except Exception as e:
            validation_results["critical_issues"].append(f"Error en validaciÃ³n: {str(e)}")
            self.logger.error(f"ğŸ’¥ Error en validaciÃ³n exhaustiva de '{phase_id}': {e}")
            return False, validation_results
    
    def _validate_infrastructure_health(self) -> bool:
        """Validar salud de infraestructura"""
        try:
            # Verificar servicios bÃ¡sicos
            services = ["clickhouse:8123/ping", "connect:8083/", "kafka:9092"]
            healthy_count = 0
            
            for service in services:
                try:
                    host, port_path = service.split(':')
                    if '/' in port_path:
                        port, path = port_path.split('/', 1)
                        url = f"http://{host}:{port}/{path}"
                    else:
                        # Para Kafka, solo verificar conectividad TCP
                        continue
                    
                    cmd = ["docker", "compose", "exec", "-T", "etl-orchestrator", "curl", "-s", "-f", url, "-m", "5"]
                    result = subprocess.run(cmd, capture_output=True, timeout=10)
                    if result.returncode == 0:
                        healthy_count += 1
                except:
                    continue
            
            return healthy_count >= 2  # Al menos ClickHouse y Connect
            
        except Exception as e:
            self.logger.warning(f"Error validando infraestructura: {e}")
            return False
    
    def _validate_database_connectivity(self) -> bool:
        """Validar conectividad a bases de datos"""
        try:
            success, output, error = self.execute_script_in_container(
                container="etl-orchestrator",
                script="tools/verify_dependencies.py",
                timeout=30
            )
            return success
        except Exception:
            return False
    
    def _validate_schema_discovery(self) -> bool:
        """Validar que el descubrimiento de esquemas fue exitoso"""
        try:
            # Verificar que se generaron archivos de configuraciÃ³n
            generated_dir = Path("/mnt/c/proyectos/etl_prod/generated/default")
            if not generated_dir.exists():
                return False
            
            # Buscar archivos de configuraciÃ³n generados
            config_files = list(generated_dir.glob("*.json"))
            return len(config_files) > 0
            
        except Exception:
            return False
    
    def _validate_clickhouse_models(self) -> bool:
        """Validar modelos de ClickHouse"""
        try:
            success, output, error = self.execute_script_in_container(
                container="etl-tools",
                script="tools/validate_clickhouse.py",
                timeout=60
            )
            return success
        except Exception:
            return False
    
    def _validate_connectors(self) -> bool:
        """Validar estado de conectores Debezium"""
        try:
            # Verificar conectores via API de Kafka Connect
            cmd = [
                "docker", "compose", "exec", "-T", "etl-orchestrator",
                "curl", "-s", "http://connect:8083/connectors", "-m", "10"
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)
            
            if result.returncode == 0:
                try:
                    connectors = json.loads(result.stdout)
                    return len(connectors) > 0  # Al menos un conector
                except json.JSONDecodeError:
                    return False
            return False
            
        except Exception:
            return False
    
    def _validate_data_flow(self) -> bool:
        """Validar flujo completo de datos"""
        try:
            success, output, error = self.execute_script_in_container(
                container="etl-tools",
                script="tools/pipeline_status.py",
                timeout=90
            )
            
            # Analizar output para verificar flujo de datos
            if success and output:
                # Buscar indicadores de datos fluyendo
                data_indicators = ["data flowing", "records processed", "tables synchronized"]
                return any(indicator in output.lower() for indicator in data_indicators)
            
            return False
            
        except Exception:
            return False
    
    def execute_phase(self, phase_id: str) -> bool:
        """Ejecutar una fase especÃ­fica con reintentos"""
        config = self.phases_config[phase_id]
        result = self.phase_results[phase_id]
        
        self.logger.info(f"\n{'='*80}")
        self.logger.info(f"ğŸš€ INICIANDO FASE: {config['name']}")
        self.logger.info(f"ğŸ“‹ DescripciÃ³n: {config['description']}")
        self.logger.info(f"ğŸ³ Contenedor: {config['container']}")
        self.logger.info(f"ğŸ”§ Script: {config['script']}")
        self.logger.info(f"{'='*80}")
        
        # Verificar dependencias
        if not self.check_dependencies(phase_id):
            result.status = PhaseStatus.FAILED
            result.error = "Dependencias no satisfechas"
            result.dependencies_met = False
            self.logger.error(f"âŒ FASE FALLÃ“: {config['name']} - Dependencias no satisfechas")
            return False
        
        result.dependencies_met = True
        result.status = PhaseStatus.RUNNING
        result.start_time = datetime.now()
        
        # Intentar ejecuciÃ³n con reintentos
        max_retries = config.get("max_retries", 1)
        
        for attempt in range(max_retries + 1):
            if attempt > 0:
                self.logger.warning(f"ğŸ”„ Reintento {attempt}/{max_retries} para '{config['name']}'")
                time.sleep(10)  # Esperar entre reintentos
            
            result.retry_count = attempt
            
            # Ejecutar script principal
            success, output, error = self.execute_script_in_container(
                container=config["container"],
                script=config["script"],
                timeout=config.get("timeout", 120)
            )
            
            if success:
                result.output = output[-1000:] if output else ""  # Guardar Ãºltimos 1000 chars
                
                # Ejecutar validaciÃ³n si estÃ¡ definida
                validation_passed = self.validate_phase(phase_id)
                result.validation_passed = validation_passed
                
                if validation_passed or not config.get("required", True):
                    # Ã‰xito completo
                    result.status = PhaseStatus.COMPLETED
                    result.end_time = datetime.now()
                    result.duration_seconds = (result.end_time - result.start_time).total_seconds()
                    
                    self.logger.info(f"âœ… FASE COMPLETADA: {config['name']} ({result.duration_seconds:.1f}s)")
                    self.save_state()
                    return True
                else:
                    # Script exitoso pero validaciÃ³n fallÃ³
                    if attempt < max_retries:
                        self.logger.warning(f"âš ï¸ ValidaciÃ³n fallÃ³, reintentando...")
                        continue
                    else:
                        result.status = PhaseStatus.FAILED
                        result.error = "ValidaciÃ³n fallÃ³ despuÃ©s de todos los reintentos"
                        break
            else:
                # Script fallÃ³
                result.error = error[-500:] if error else "Script fallÃ³ sin mensaje de error"
                
                # Auto-reparaciÃ³n para errores de permisos Docker
                if "permission denied while trying to connect to the Docker daemon socket" in (error or ""):
                    self.logger.warning("ğŸ”§ Error de permisos en Docker socket detectado. Ejecutando reparaciÃ³n automÃ¡tica...")
                    try:
                        # Ejecutar como root para permisos de sistema
                        repair_result = subprocess.run([
                            "docker", "compose", "run", "--rm", "--user", "root", "etl-orchestrator", 
                            "python3", "tools/fix_docker_socket.py"
                        ], capture_output=True, text=True, timeout=60)
                        
                        if repair_result.returncode == 0:
                            self.logger.info("âœ… ReparaciÃ³n automÃ¡tica de Docker socket ejecutada. Reintentando...")
                        else:
                            self.logger.error(f"âŒ ReparaciÃ³n automÃ¡tica fallÃ³: {repair_result.stderr[:200]}")
                    except Exception as e:
                        self.logger.error(f"âŒ Error ejecutando reparaciÃ³n automÃ¡tica: {e}")
                
                if attempt < max_retries:
                    self.logger.warning(f"âš ï¸ Script fallÃ³, reintentando: {result.error[:100]}")
                    continue
                else:
                    result.status = PhaseStatus.FAILED
                    break
        
        # FallÃ³ despuÃ©s de todos los reintentos
        result.end_time = datetime.now()
        result.duration_seconds = (result.end_time - result.start_time).total_seconds()
        
        # Decidir si continuar o abortar
        if config.get("required", True):
            self.logger.error(f"âŒ FASE CRÃTICA FALLÃ“: {config['name']}")
            self.logger.error(f"ğŸ’¥ Error: {result.error}")
            return False
        else:
            result.status = PhaseStatus.SKIPPED
            self.logger.warning(f"âš ï¸ FASE OPCIONAL FALLÃ“: {config['name']} - Continuando")
            return True
    
    def wait_for_infrastructure(self) -> bool:
        """
        Espera que la infraestructura estÃ© lista usando el validador avanzado de salud.
        Si detecta error de autenticaciÃ³n en ClickHouse, ejecuta fix_clickhouse_config.py en etl-tools y reintenta.
        """
        self.logger.info("â³ Validando conectividad de red entre servicios crÃ­ticos...")
        net_result = subprocess.run([
            "python3", "tools/network_validator.py"
        ], capture_output=True, text=True, timeout=60)
        net_output = net_result.stdout + net_result.stderr
        try:
            with open("/app/logs/network_check_results.json") as f:
                net_health = json.load(f)
            if net_health.get("overall_status") != "healthy":
                self.logger.error(f"âŒ Fallo de red detectado: {net_health.get('errors')}")
                return False
            else:
                self.logger.info("âœ… Conectividad de red OK. Continuando con validaciÃ³n de infraestructura...")
        except Exception as e:
            self.logger.error(f"âŒ No se pudo leer network_check_results.json: {e}")
            return False
        max_wait = 300
        start_time = time.time()
        repair_attempted = False
        while (time.time() - start_time) < max_wait:
            try:
                # Ejecutar health_validator.py directamente
                result = subprocess.run([
                    "python3", "tools/health_validator.py"
                ], capture_output=True, text=True, timeout=60)
                output = result.stdout + result.stderr
                # Detectar error de permisos Docker
                if "permission denied while trying to connect to the Docker daemon socket" in output:
                    self.logger.warning("ğŸ”§ Error de permisos en Docker socket detectado. Ejecutando reparaciÃ³n automÃ¡tica...")
                    # Ejecutar como root para permisos de sistema
                    result = subprocess.run([
                        "docker", "compose", "run", "--rm", "--user", "root", "etl-orchestrator", 
                        "python3", "tools/fix_docker_socket.py"
                    ], capture_output=True, text=True, timeout=60)
                    success = result.returncode == 0
                    out = result.stdout
                    err = result.stderr
                    if success:
                        self.logger.info("âœ… ReparaciÃ³n automÃ¡tica de Docker socket ejecutada. Reintentando validaciÃ³n...")
                    else:
                        self.logger.error(f"âŒ ReparaciÃ³n automÃ¡tica Docker socket fallÃ³: {err[:200]}")
                    time.sleep(5)
                    continue
                if result.returncode == 0:
                    # Parsear el resultado JSON
                    try:
                        with open("/app/logs/health_check_results.json") as f:
                            health = json.load(f)
                        if health.get("overall_status") in ["fully_healthy", "mostly_healthy"]:
                            self.logger.info(f"âœ… Infraestructura lista ({time.time()-start_time:.1f}s)")
                            return True
                        else:
                            self.logger.info(f"â³ Infraestructura aÃºn no healthy: {health.get('overall_status')}")
                            # Detectar error de autenticaciÃ³n ClickHouse
                            if (not repair_attempted and health.get("critical_issues") and any("AUTHENTICATION_FAILED" in err or "Authentication failed" in err for err in health.get("critical_issues", []))):
                                self.logger.warning("ğŸ”§ Error de autenticaciÃ³n ClickHouse detectado. Ejecutando reparaciÃ³n automÃ¡tica...")
                                success, out, err = self.execute_script_in_container(
                                    container="etl-tools",
                                    script="tools/fix_clickhouse_config.py",
                                    timeout=90
                                )
                                if success:
                                    self.logger.info("âœ… ReparaciÃ³n automÃ¡tica de ClickHouse ejecutada. Reintentando validaciÃ³n...")
                                else:
                                    self.logger.error(f"âŒ ReparaciÃ³n automÃ¡tica fallÃ³: {err[:200]}")
                                repair_attempted = True
                    except Exception as e:
                        self.logger.warning(f"No se pudo leer health_check_results.json: {e}")
                else:
                    self.logger.info(f"â³ health_validator.py aÃºn no retorna healthy")
                    # Detectar error de autenticaciÃ³n en output si no hay JSON
                    if (not repair_attempted and ("AUTHENTICATION_FAILED" in output or "Authentication failed" in output)):
                        self.logger.warning("ğŸ”§ Error de autenticaciÃ³n ClickHouse detectado. Ejecutando reparaciÃ³n automÃ¡tica...")
                        success, out, err = self.execute_script_in_container(
                            container="etl-tools",
                            script="tools/fix_clickhouse_config.py",
                            timeout=90
                        )
                        if success:
                            self.logger.info("âœ… ReparaciÃ³n automÃ¡tica de ClickHouse ejecutada. Reintentando validaciÃ³n...")
                        else:
                            self.logger.error(f"âŒ ReparaciÃ³n automÃ¡tica fallÃ³: {err[:200]}")
                        repair_attempted = True
            except Exception as e:
                self.logger.warning(f"Error ejecutando health_validator: {e}")
            time.sleep(10)
        self.logger.error(f"âŒ Infraestructura no disponible despuÃ©s de {max_wait}s (health_validator)")
        return False
    
    def run_orchestration(self) -> bool:
        """Ejecutar orquestaciÃ³n completa"""
        self.logger.info("ğŸ¯ === INICIANDO ORQUESTACIÃ“N MAESTRA DEL PIPELINE ETL ===")
        self.logger.info(f"â° Inicio: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info(f"ğŸ“‹ Fases programadas: {len(self.phases_config)}")
        
        try:
            # 1. Esperar infraestructura bÃ¡sica
            if not self.wait_for_infrastructure():
                self.logger.error("âŒ Infraestructura no disponible, abortando")
                return False
            
            # 2. Ejecutar fases en orden topolÃ³gico
            phases_order = [
                "infrastructure_validation",
                "user_setup", 
                "schema_discovery",
                "clickhouse_models",
                "connector_generation",
                "connector_deployment", 
                "data_ingestion",
                "superset_setup"
            ]
            
            successful_phases = 0
            
            for phase_id in phases_order:
                if self.execute_phase(phase_id):
                    successful_phases += 1
                else:
                    config = self.phases_config[phase_id]
                    if config.get("required", True):
                        self.logger.error(f"âŒ Fase crÃ­tica '{phase_id}' fallÃ³, abortando orquestaciÃ³n")
                        return False
                    else:
                        self.logger.warning(f"âš ï¸ Fase opcional '{phase_id}' fallÃ³, continuando")
                
                # Guardar estado despuÃ©s de cada fase
                self.save_state()
            
            # 3. Verificar Ã©xito general
            total_phases = len(phases_order)
            required_phases = sum(1 for p in phases_order if self.phases_config[p].get("required", True))
            completed_required = sum(
                1 for p in phases_order 
                if self.phases_config[p].get("required", True) 
                and self.phase_results[p].status == PhaseStatus.COMPLETED
            )
            
            if completed_required == required_phases:
                self.logger.info(f"ğŸ‰ ORQUESTACIÃ“N EXITOSA: {successful_phases}/{total_phases} fases completadas")
                self.logger.info(f"âœ… Todas las fases crÃ­ticas completadas ({completed_required}/{required_phases})")
                return True
            else:
                self.logger.error(f"âŒ ORQUESTACIÃ“N FALLÃ“: Solo {completed_required}/{required_phases} fases crÃ­ticas completadas")
                return False
            
        except Exception as e:
            self.logger.error(f"ğŸ’¥ Error crÃ­tico en orquestaciÃ³n: {str(e)}")
            return False
        
        finally:
            self.save_state()
            self.print_final_summary()
    
    def print_final_summary(self):
        """Imprimir resumen final detallado"""
        end_time = datetime.now()
        total_duration = (end_time - self.start_time).total_seconds()
        
        print(f"\n{'='*100}")
        print(f"ğŸ RESUMEN FINAL DE ORQUESTACIÃ“N MAESTRA")
        print(f"{'='*100}")
        print(f"â° Inicio: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â° Fin: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â±ï¸  DuraciÃ³n total: {total_duration:.1f} segundos")
        
        # EstadÃ­sticas por estado
        status_counts = {}
        for result in self.phase_results.values():
            status = result.status.value
            status_counts[status] = status_counts.get(status, 0) + 1
        
        print(f"\nğŸ“Š ESTADÃSTICAS DE FASES:")
        print(f"   âœ… Completadas: {status_counts.get('completed', 0)}")
        print(f"   âŒ Fallidas: {status_counts.get('failed', 0)}")
        print(f"   âš ï¸  Saltadas: {status_counts.get('skipped', 0)}")
        print(f"   â³ Pendientes: {status_counts.get('pending', 0)}")
        
        # Detalle de cada fase
        print(f"\nğŸ“‹ DETALLE DE FASES:")
        for phase_id, result in self.phase_results.items():
            status_icon = {
                PhaseStatus.COMPLETED: "âœ…",
                PhaseStatus.FAILED: "âŒ", 
                PhaseStatus.SKIPPED: "âš ï¸",
                PhaseStatus.PENDING: "â³",
                PhaseStatus.RUNNING: "ğŸ”„"
            }.get(result.status, "â“")
            
            print(f"   {status_icon} {result.name}")
            if result.duration_seconds > 0:
                print(f"      â±ï¸  DuraciÃ³n: {result.duration_seconds:.1f}s")
            if result.retry_count > 0:
                print(f"      ğŸ”„ Reintentos: {result.retry_count}")
            if result.error:
                print(f"      ğŸ’¥ Error: {result.error[:80]}...")
        
        # PrÃ³ximos pasos
        completed_count = status_counts.get('completed', 0)
        failed_count = status_counts.get('failed', 0)
        
        print(f"\nğŸ’¡ PRÃ“XIMOS PASOS:")
        if completed_count > failed_count and failed_count == 0:
            print(f"   1. âœ… Pipeline completamente operativo")
            print(f"   2. ğŸ” Verificar datos: python3 tools/pipeline_status.py")
            print(f"   3. ğŸ“Š Acceder a Superset: http://localhost:8088")
            print(f"   4. ğŸ“ˆ Configurar monitoreo de producciÃ³n")
        elif completed_count > failed_count:
            print(f"   1. âš ï¸  Pipeline parcialmente operativo")
            print(f"   2. ğŸ”§ Revisar fases fallidas en logs")
            print(f"   3. ğŸ”„ Ejecutar reintentos manuales si es necesario")
        else:
            print(f"   1. âŒ Pipeline no operativo")
            print(f"   2. ğŸ” Revisar logs detallados en /app/logs/")
            print(f"   3. ğŸ› ï¸  Corregir errores de infraestructura")
            print(f"   4. ğŸ”„ Re-ejecutar orquestaciÃ³n")
        
        print(f"{'='*100}")

def main():
    """FunciÃ³n principal del orquestador maestro"""
    try:
        # Banner de inicio
        print(f"\n{'='*100}")
        print(f"ğŸ¯ ORQUESTADOR MAESTRO CENTRALIZADO DEL PIPELINE ETL")
        print(f"ğŸš€ CoordinaciÃ³n completa con validaciones entre fases")
        print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*100}")
        
        orchestrator = MasterOrchestrator()
        success = orchestrator.run_orchestration()
        
        return 0 if success else 1
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ OrquestaciÃ³n interrumpida por el usuario")
        return 1
    except Exception as e:
        print(f"ğŸ’¥ Error crÃ­tico en main: {str(e)}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)