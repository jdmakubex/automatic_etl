#!/usr/bin/env python3
"""
Script para crear datasets usando el contexto de aplicaciÃ³n de Superset
"""

# Importar los mÃ³dulos necesarios
from superset.app import create_app
from superset import db
from superset.models.core import Database
from superset.connectors.sqla.models import SqlaTable, TableColumn
from sqlalchemy import inspect
import logging

# Configurar logging bÃ¡sico
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Crear contexto de aplicaciÃ³n
app = create_app()

with app.app_context():
    try:
        print("ğŸ” Buscando base de datos ClickHouse...")
        
        # Buscar la base de datos ClickHouse
        clickhouse_db = db.session.query(Database).filter(
            Database.database_name.like('%ClickHouse%')
        ).first()
        
        if not clickhouse_db:
            print("âŒ Base de datos ClickHouse no encontrada")
            # Listar todas las bases de datos disponibles
            all_dbs = db.session.query(Database).all()
            print("ğŸ“‹ Bases de datos disponibles:")
            for db_item in all_dbs:
                print(f"  - {db_item.database_name}")
            exit(1)
        
        print(f"âœ… Base de datos encontrada: {clickhouse_db.database_name} (ID: {clickhouse_db.id})")
        print(f"ğŸ”— URI: {clickhouse_db.sqlalchemy_uri}")
        
        # Obtener inspector para la base de datos
        try:
            engine = clickhouse_db.get_sqla_engine()
            inspector = inspect(engine)
            
            # Obtener tablas del esquema
            schema_name = 'fgeo_analytics'
            print(f"ğŸ“‹ Buscando tablas en esquema: {schema_name}")
            
            table_names = inspector.get_table_names(schema=schema_name)
            if not table_names:
                # Intentar sin esquema especÃ­fico
                table_names = inspector.get_table_names()
                schema_name = None
                print("ğŸ“‹ Tablas encontradas (esquema por defecto):")
            else:
                print(f"ğŸ“‹ Tablas encontradas en {schema_name}:")
            
            for table_name in table_names:
                print(f"  - {table_name}")
            
            if not table_names:
                print("âŒ No se encontraron tablas")
                exit(1)
            
            # Crear datasets para cada tabla
            created_count = 0
            updated_count = 0
            
            for table_name in table_names:
                print(f"\nğŸ”„ Procesando tabla: {table_name}")
                
                # Verificar si el dataset ya existe
                existing_dataset = db.session.query(SqlaTable).filter(
                    SqlaTable.table_name == table_name,
                    SqlaTable.database_id == clickhouse_db.id
                ).first()
                
                if existing_dataset:
                    print(f"âš ï¸  Dataset '{table_name}' ya existe, actualizando...")
                    dataset = existing_dataset
                    updated_count += 1
                else:
                    # Crear nuevo dataset
                    dataset = SqlaTable(
                        table_name=table_name,
                        database=clickhouse_db,
                        schema=schema_name
                    )
                    db.session.add(dataset)
                    db.session.flush()  # Para obtener el ID
                    print(f"âœ… Dataset '{table_name}' creado")
                    created_count += 1
                
                # Obtener columnas de la tabla
                try:
                    columns = inspector.get_columns(table_name, schema=schema_name)
                    print(f"  ğŸ“„ Encontradas {len(columns)} columnas")
                    
                    # Crear/actualizar columnas
                    for col_info in columns:
                        col_name = col_info['name']
                        col_type = str(col_info['type'])
                        
                        existing_column = db.session.query(TableColumn).filter(
                            TableColumn.table_id == dataset.id,
                            TableColumn.column_name == col_name
                        ).first()
                        
                        if not existing_column:
                            column = TableColumn(
                                column_name=col_name,
                                type=col_type,
                                table=dataset
                            )
                            db.session.add(column)
                            print(f"    + {col_name} ({col_type})")
                        else:
                            print(f"    âœ“ {col_name} ({col_type})")
                    
                except Exception as col_error:
                    print(f"  âš ï¸  Error obteniendo columnas: {col_error}")
            
            # Commit los cambios
            try:
                db.session.commit()
                print(f"\nâœ… ConfiguraciÃ³n completada:")
                print(f"   ğŸ“Š Datasets creados: {created_count}")
                print(f"   ğŸ”„ Datasets actualizados: {updated_count}")
                print(f"   ğŸ“‹ Total procesados: {created_count + updated_count}")
                
            except Exception as commit_error:
                print(f"âŒ Error al guardar datasets: {commit_error}")
                db.session.rollback()
                exit(1)
                
        except Exception as engine_error:
            print(f"âŒ Error conectando a ClickHouse: {engine_error}")
            exit(1)
            
    except Exception as general_error:
        print(f"âŒ Error general: {general_error}")
        exit(1)

print("ğŸ‰ Â¡ConfiguraciÃ³n de datasets completada exitosamente!")